{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import os\n",
    "import gym\n",
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'CartPole-v1'\n",
    "# Generate environment (currently only one supported by PyTorch)\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# episodes = 5  # number of episodes to test\n",
    "\n",
    "# for episode in range(1, episodes+1):\n",
    "#     state = env.reset()  # reset the environment get the initial state\n",
    "#     done = False  # set the episode end flag to false\n",
    "#     score = 0\n",
    "\n",
    "#     while not done:\n",
    "#         env.render()  # render the environment\n",
    "#         # sample an action from the action space (e.g. a random action)\n",
    "#         action = env.action_space.sample()\n",
    "#         # take the action and get the next state, reward and episode end flag from the environment\n",
    "#         state, reward, done, info = env.step(action)\n",
    "\n",
    "#         score += reward  # update the score (sum of rewards)\n",
    "#     print(f'Episode: {episode} Score: {score}')\n",
    "\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the log folder (where the model will be saved)\n",
    "log_path = os.path.join('Training', 'Logs')\n",
    "# create a vectorized environment to run the agent in parallel\n",
    "env = DummyVecEnv([lambda: env])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "model = PPO(\"MlpPolicy\", env, verbose=1,\n",
    "            tensorboard_log=log_path)  # create the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.learn(total_timesteps=30000)  # train the agent (using the environment) for 30000 timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPO_path = os.path.join('Training','Saved Models', 'PPO_model_Cartpole')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\Logs\\PPO_5\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 379  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 5    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 512         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008411117 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.686      |\n",
      "|    explained_variance   | -0.00443    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.59        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0191     |\n",
      "|    value_loss           | 56          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 555         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011148293 |\n",
      "|    clip_fraction        | 0.0818      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.66       |\n",
      "|    explained_variance   | 0.0872      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.72        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0208     |\n",
      "|    value_loss           | 31.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 559         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008221744 |\n",
      "|    clip_fraction        | 0.0718      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.637      |\n",
      "|    explained_variance   | 0.213       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 20.5        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.016      |\n",
      "|    value_loss           | 52.4        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 581          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 17           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076443073 |\n",
      "|    clip_fraction        | 0.06         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.617       |\n",
      "|    explained_variance   | 0.271        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 24.1         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.0168      |\n",
      "|    value_loss           | 61.6         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 604         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 20          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010192037 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.586      |\n",
      "|    explained_variance   | 0.502       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 27.9        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.018      |\n",
      "|    value_loss           | 59.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 616         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 23          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008432595 |\n",
      "|    clip_fraction        | 0.0739      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.605      |\n",
      "|    explained_variance   | 0.684       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 20.5        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00834    |\n",
      "|    value_loss           | 43.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 629         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005541877 |\n",
      "|    clip_fraction        | 0.0496      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.592      |\n",
      "|    explained_variance   | 0.678       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 17.5        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00682    |\n",
      "|    value_loss           | 49.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 613         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005915486 |\n",
      "|    clip_fraction        | 0.0276      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.566      |\n",
      "|    explained_variance   | 0.399       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10          |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00582    |\n",
      "|    value_loss           | 67          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 616         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 33          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008484307 |\n",
      "|    clip_fraction        | 0.0665      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.571      |\n",
      "|    explained_variance   | 0.846       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.87        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00776    |\n",
      "|    value_loss           | 33.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 624         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 36          |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005966755 |\n",
      "|    clip_fraction        | 0.0418      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.561      |\n",
      "|    explained_variance   | 0.624       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.78        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00354    |\n",
      "|    value_loss           | 47.2        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 626          |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 39           |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0082387645 |\n",
      "|    clip_fraction        | 0.06         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.551       |\n",
      "|    explained_variance   | 0.393        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.18         |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.00675     |\n",
      "|    value_loss           | 81.9         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 636         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 41          |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004312888 |\n",
      "|    clip_fraction        | 0.0364      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.538      |\n",
      "|    explained_variance   | 0.0687      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 41.2        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.000852   |\n",
      "|    value_loss           | 82.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 631          |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 45           |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039014667 |\n",
      "|    clip_fraction        | 0.0258       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.536       |\n",
      "|    explained_variance   | 0.256        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 61.2         |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.00111     |\n",
      "|    value_loss           | 77.9         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 634         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 48          |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003134505 |\n",
      "|    clip_fraction        | 0.0171      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.527      |\n",
      "|    explained_variance   | 0.357       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 37.2        |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.00248    |\n",
      "|    value_loss           | 66.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 642          |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 51           |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028416773 |\n",
      "|    clip_fraction        | 0.0333       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.511       |\n",
      "|    explained_variance   | 0.0971       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 35.7         |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.0016      |\n",
      "|    value_loss           | 96.6         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 632          |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 55           |\n",
      "|    total_timesteps      | 34816        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011706196 |\n",
      "|    clip_fraction        | 0.00547      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.505       |\n",
      "|    explained_variance   | 0.352        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 19.8         |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.00315     |\n",
      "|    value_loss           | 79.3         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 629          |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 58           |\n",
      "|    total_timesteps      | 36864        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024848767 |\n",
      "|    clip_fraction        | 0.00825      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.506       |\n",
      "|    explained_variance   | 0.129        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 60.9         |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.000778    |\n",
      "|    value_loss           | 112          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 633          |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 61           |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058112433 |\n",
      "|    clip_fraction        | 0.0336       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.507       |\n",
      "|    explained_variance   | 0.19         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.39         |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.00259     |\n",
      "|    value_loss           | 73.3         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 632          |\n",
      "|    iterations           | 20           |\n",
      "|    time_elapsed         | 64           |\n",
      "|    total_timesteps      | 40960        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031754447 |\n",
      "|    clip_fraction        | 0.0212       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.503       |\n",
      "|    explained_variance   | 0.283        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 43.2         |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.00247     |\n",
      "|    value_loss           | 88.2         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 631         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 68          |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007825473 |\n",
      "|    clip_fraction        | 0.0711      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.491      |\n",
      "|    explained_variance   | 0.463       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 68.9        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.00789    |\n",
      "|    value_loss           | 116         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 630          |\n",
      "|    iterations           | 22           |\n",
      "|    time_elapsed         | 71           |\n",
      "|    total_timesteps      | 45056        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045491364 |\n",
      "|    clip_fraction        | 0.0246       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.509       |\n",
      "|    explained_variance   | 0.3          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.91         |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.00198     |\n",
      "|    value_loss           | 87.7         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 634          |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 74           |\n",
      "|    total_timesteps      | 47104        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035733087 |\n",
      "|    clip_fraction        | 0.0552       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.492       |\n",
      "|    explained_variance   | 0.184        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 117          |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.00182     |\n",
      "|    value_loss           | 99.9         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 637          |\n",
      "|    iterations           | 24           |\n",
      "|    time_elapsed         | 77           |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046110004 |\n",
      "|    clip_fraction        | 0.0289       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.491       |\n",
      "|    explained_variance   | -0.0168      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 90.7         |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.000469    |\n",
      "|    value_loss           | 116          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 638         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 80          |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005133449 |\n",
      "|    clip_fraction        | 0.0422      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.495      |\n",
      "|    explained_variance   | 0.00465     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 35.2        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.00184    |\n",
      "|    value_loss           | 111         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 637          |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 83           |\n",
      "|    total_timesteps      | 53248        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050112065 |\n",
      "|    clip_fraction        | 0.0376       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.5         |\n",
      "|    explained_variance   | -0.00298     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 46.9         |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -0.00171     |\n",
      "|    value_loss           | 110          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 640          |\n",
      "|    iterations           | 27           |\n",
      "|    time_elapsed         | 86           |\n",
      "|    total_timesteps      | 55296        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035292248 |\n",
      "|    clip_fraction        | 0.0259       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.504       |\n",
      "|    explained_variance   | 0.0527       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 19.3         |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | -0.00183     |\n",
      "|    value_loss           | 109          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 641         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 89          |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003566091 |\n",
      "|    clip_fraction        | 0.0377      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.518      |\n",
      "|    explained_variance   | -0.0172     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 62.8        |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -6.28e-05   |\n",
      "|    value_loss           | 119         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 643         |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 92          |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002926013 |\n",
      "|    clip_fraction        | 0.00869     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.512      |\n",
      "|    explained_variance   | 0.0218      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 20.4        |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.000465   |\n",
      "|    value_loss           | 98.7        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 647          |\n",
      "|    iterations           | 30           |\n",
      "|    time_elapsed         | 94           |\n",
      "|    total_timesteps      | 61440        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036936789 |\n",
      "|    clip_fraction        | 0.0292       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.494       |\n",
      "|    explained_variance   | 0.0805       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 24.2         |\n",
      "|    n_updates            | 290          |\n",
      "|    policy_gradient_loss | -0.00347     |\n",
      "|    value_loss           | 131          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 649          |\n",
      "|    iterations           | 31           |\n",
      "|    time_elapsed         | 97           |\n",
      "|    total_timesteps      | 63488        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056333113 |\n",
      "|    clip_fraction        | 0.0364       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.509       |\n",
      "|    explained_variance   | 0.397        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 41.6         |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | -0.00194     |\n",
      "|    value_loss           | 85.7         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 653          |\n",
      "|    iterations           | 32           |\n",
      "|    time_elapsed         | 100          |\n",
      "|    total_timesteps      | 65536        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044082534 |\n",
      "|    clip_fraction        | 0.0401       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.529       |\n",
      "|    explained_variance   | -0.0289      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 33.2         |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | -0.00205     |\n",
      "|    value_loss           | 126          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 654          |\n",
      "|    iterations           | 33           |\n",
      "|    time_elapsed         | 103          |\n",
      "|    total_timesteps      | 67584        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031993748 |\n",
      "|    clip_fraction        | 0.0184       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.506       |\n",
      "|    explained_variance   | 0.0154       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 27.5         |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.000737    |\n",
      "|    value_loss           | 112          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 655         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 106         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002931341 |\n",
      "|    clip_fraction        | 0.0105      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.514      |\n",
      "|    explained_variance   | 0.147       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 26.9        |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.00185    |\n",
      "|    value_loss           | 65.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 653         |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 109         |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003830065 |\n",
      "|    clip_fraction        | 0.0379      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.528      |\n",
      "|    explained_variance   | 0.103       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 61.1        |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.003      |\n",
      "|    value_loss           | 111         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 656         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 112         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002622607 |\n",
      "|    clip_fraction        | 0.0206      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.521      |\n",
      "|    explained_variance   | 0.324       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 71          |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.00151    |\n",
      "|    value_loss           | 100         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 658         |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 115         |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005197956 |\n",
      "|    clip_fraction        | 0.0501      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.515      |\n",
      "|    explained_variance   | 0.312       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.4        |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.00408    |\n",
      "|    value_loss           | 102         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 658          |\n",
      "|    iterations           | 38           |\n",
      "|    time_elapsed         | 118          |\n",
      "|    total_timesteps      | 77824        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042280587 |\n",
      "|    clip_fraction        | 0.0217       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.517       |\n",
      "|    explained_variance   | 0.42         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 61.3         |\n",
      "|    n_updates            | 370          |\n",
      "|    policy_gradient_loss | -0.00201     |\n",
      "|    value_loss           | 119          |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x219ec736820>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=77000)  # train the agent (using the environment) for 77000 timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(PPO_path)  # save the agent (weights, etc.) to a file (to be loaded later) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sajal\\AppData\\Roaming\\Python\\Python39\\site-packages\\stable_baselines3\\common\\evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(500.0, 0.0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=10,render=True)  # evaluate the agent in the environment using 10 episodes (randomly sampled) and render the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close() # close the environment (and free memory) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 Score: [500.]\n",
      "Episode: 2 Score: [500.]\n",
      "Episode: 3 Score: [500.]\n",
      "Episode: 4 Score: [500.]\n",
      "Episode: 5 Score: [500.]\n",
      "Episode: 6 Score: [500.]\n",
      "Episode: 7 Score: [500.]\n",
      "Episode: 8 Score: [500.]\n",
      "Episode: 9 Score: [500.]\n",
      "Episode: 10 Score: [500.]\n"
     ]
    }
   ],
   "source": [
    "episodes = 10  # number of episodes to test\n",
    "\n",
    "for episode in range(1, episodes+1):\n",
    "    obs = env.reset()  # reset the environment get the initial state\n",
    "    done = False  # set the episode end flag to false\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        env.render()  # render the environment\n",
    "\n",
    "        # Model.predict() returns the action to take from the state (in this case the state is the observation)\n",
    "        action, _ = model.predict(obs)\n",
    "\n",
    "        obs, reward, done, info = env.step(action)\n",
    "\n",
    "        score += reward  # update the score (sum of rewards)\n",
    "    print(f'Episode: {episode} Score: {score}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Open and run tensorboard run the following CMD on terminal\n",
    "!tensorboard --logdir=Training/Logs/PPO_1/ --port=6006 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import EvalCallback,StopTrainingOnRewardThreshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join('Training','Saved Models') # path to the folder where the model will be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_callback = StopTrainingOnRewardThreshold(reward_threshold=400, verbose=1)\n",
    "eval_callback = EvalCallback(env,callback_on_new_best=stop_callback,eval_freq=10000,best_model_save_path= save_path,verbose=1) # create the callback (to be called at every 10000 timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Logging to Training\\Logs\\PPO_6\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1301 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 999         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009390707 |\n",
      "|    clip_fraction        | 0.098       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.686      |\n",
      "|    explained_variance   | -0.000103   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.12        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0159     |\n",
      "|    value_loss           | 55.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 937         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010872097 |\n",
      "|    clip_fraction        | 0.0889      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.669      |\n",
      "|    explained_variance   | 0.0857      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12          |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0229     |\n",
      "|    value_loss           | 28.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 874         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010391124 |\n",
      "|    clip_fraction        | 0.09        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.633      |\n",
      "|    explained_variance   | 0.183       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 22.9        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0192     |\n",
      "|    value_loss           | 53.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=341.00 +/- 129.89\n",
      "Episode length: 341.00 +/- 129.89\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 341          |\n",
      "|    mean_reward          | 341          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 10000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067709787 |\n",
      "|    clip_fraction        | 0.0532       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.609       |\n",
      "|    explained_variance   | 0.238        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 30.3         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.0142      |\n",
      "|    value_loss           | 66.4         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 817   |\n",
      "|    iterations      | 5     |\n",
      "|    time_elapsed    | 12    |\n",
      "|    total_timesteps | 10240 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 825         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006666206 |\n",
      "|    clip_fraction        | 0.0554      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.583      |\n",
      "|    explained_variance   | 0.414       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.6        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0127     |\n",
      "|    value_loss           | 60.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 827         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 17          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006937662 |\n",
      "|    clip_fraction        | 0.0314      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.577      |\n",
      "|    explained_variance   | 0.562       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.9        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00722    |\n",
      "|    value_loss           | 61.7        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 833          |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 19           |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054249153 |\n",
      "|    clip_fraction        | 0.0644       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.574       |\n",
      "|    explained_variance   | 0.743        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.91         |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.0077      |\n",
      "|    value_loss           | 48           |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 825         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 22          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011010308 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.565      |\n",
      "|    explained_variance   | 0.886       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.16        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0129     |\n",
      "|    value_loss           | 27.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 20000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058034332 |\n",
      "|    clip_fraction        | 0.0427       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.555       |\n",
      "|    explained_variance   | 0.37         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.7         |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.0052      |\n",
      "|    value_loss           | 56.7         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Stopping training because the mean reward 500.00  is above the threshold 400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x21981149e20>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PPO(\"MlpPolicy\", env, verbose=1,tensorboard_log=log_path)  # create the agent\n",
    "model.learn(total_timesteps=60000,callback=eval_callback)  # train the agent (using the environment) for 30000 timesteps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changing Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_arch = [dict(pi=[128,128,128,128],vf=[128,128,128,128])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "model = PPO(\"MlpPolicy\", env, verbose=1,tensorboard_log=log_path, policy_kwargs={'net_arch':net_arch})  # create the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\Logs\\PPO_7\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1102 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 684         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013566804 |\n",
      "|    clip_fraction        | 0.215       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.682      |\n",
      "|    explained_variance   | 0.00369     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.3         |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0254     |\n",
      "|    value_loss           | 21.3        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 606          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0132970875 |\n",
      "|    clip_fraction        | 0.154        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.65        |\n",
      "|    explained_variance   | 0.386        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 11           |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.0249      |\n",
      "|    value_loss           | 28.7         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 560         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012718786 |\n",
      "|    clip_fraction        | 0.176       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.612      |\n",
      "|    explained_variance   | 0.529       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 20          |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0296     |\n",
      "|    value_loss           | 44.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=389.20 +/- 92.57\n",
      "Episode length: 389.20 +/- 92.57\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 389         |\n",
      "|    mean_reward          | 389         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013904517 |\n",
      "|    clip_fraction        | 0.198       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.579      |\n",
      "|    explained_variance   | 0.526       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.5        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.025      |\n",
      "|    value_loss           | 40.4        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 512   |\n",
      "|    iterations      | 5     |\n",
      "|    time_elapsed    | 19    |\n",
      "|    total_timesteps | 10240 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 515         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 23          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013130077 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.555      |\n",
      "|    explained_variance   | 0.762       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 14.6        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.015      |\n",
      "|    value_loss           | 27.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 518         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008117955 |\n",
      "|    clip_fraction        | 0.0987      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.564      |\n",
      "|    explained_variance   | 0.871       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.76        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0052     |\n",
      "|    value_loss           | 18.4        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 512          |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 31           |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049757613 |\n",
      "|    clip_fraction        | 0.0435       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.556       |\n",
      "|    explained_variance   | 0.0936       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.25         |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00293     |\n",
      "|    value_loss           | 52.7         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 511         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 36          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002755273 |\n",
      "|    clip_fraction        | 0.0571      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.521      |\n",
      "|    explained_variance   | 0.204       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 31.3        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00152    |\n",
      "|    value_loss           | 57          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 20000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035589663 |\n",
      "|    clip_fraction        | 0.0608       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.539       |\n",
      "|    explained_variance   | 0.036        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.44         |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | 0.00197      |\n",
      "|    value_loss           | 73.5         |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 487   |\n",
      "|    iterations      | 10    |\n",
      "|    time_elapsed    | 41    |\n",
      "|    total_timesteps | 20480 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 492          |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 45           |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059376624 |\n",
      "|    clip_fraction        | 0.0542       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.527       |\n",
      "|    explained_variance   | 0.489        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 113          |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.00387     |\n",
      "|    value_loss           | 53.6         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 490         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 50          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007418502 |\n",
      "|    clip_fraction        | 0.0711      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.503      |\n",
      "|    explained_variance   | 0.0891      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.8        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00231    |\n",
      "|    value_loss           | 95.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 491          |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 54           |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016590166 |\n",
      "|    clip_fraction        | 0.0662       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.54        |\n",
      "|    explained_variance   | 0.771        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.68         |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00323     |\n",
      "|    value_loss           | 33.4         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 489         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 58          |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003907147 |\n",
      "|    clip_fraction        | 0.0287      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.518      |\n",
      "|    explained_variance   | 0.518       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 14.7        |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.00202    |\n",
      "|    value_loss           | 56.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 30000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032032342 |\n",
      "|    clip_fraction        | 0.0951       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.517       |\n",
      "|    explained_variance   | -0.0178      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 71.6         |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | 0.00148      |\n",
      "|    value_loss           | 107          |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 480   |\n",
      "|    iterations      | 15    |\n",
      "|    time_elapsed    | 63    |\n",
      "|    total_timesteps | 30720 |\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x219815591f0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=30000,callback=eval_callback)  # train the agent (using the environment) for 30000 timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "dqn = DQN(\"MlpPolicy\", env, verbose=1,tensorboard_log=log_path)  # create the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\Logs\\PPO_8\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 991  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 2    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 520          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 7            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049489737 |\n",
      "|    clip_fraction        | 0.0595       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.468       |\n",
      "|    explained_variance   | 0.0117       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.46         |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.00451     |\n",
      "|    value_loss           | 86.3         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 509          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030453359 |\n",
      "|    clip_fraction        | 0.0607       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.457       |\n",
      "|    explained_variance   | 0.0315       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 41.4         |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.00364     |\n",
      "|    value_loss           | 105          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 503          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047218297 |\n",
      "|    clip_fraction        | 0.0641       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.471       |\n",
      "|    explained_variance   | -0.0856      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 78.3         |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.00315     |\n",
      "|    value_loss           | 122          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 508          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 20           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048466763 |\n",
      "|    clip_fraction        | 0.0756       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.492       |\n",
      "|    explained_variance   | 0.00948      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.22         |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.00105     |\n",
      "|    value_loss           | 108          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 499          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 24           |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037429854 |\n",
      "|    clip_fraction        | 0.0562       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.483       |\n",
      "|    explained_variance   | -0.00096     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 29.3         |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.00276     |\n",
      "|    value_loss           | 99.7         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 489         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006328767 |\n",
      "|    clip_fraction        | 0.0329      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.479      |\n",
      "|    explained_variance   | 0.0762      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 60.6        |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.00317    |\n",
      "|    value_loss           | 97          |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 475          |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 34           |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066257333 |\n",
      "|    clip_fraction        | 0.0367       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.467       |\n",
      "|    explained_variance   | 0.457        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.64         |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.00147     |\n",
      "|    value_loss           | 87.8         |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 465        |\n",
      "|    iterations           | 9          |\n",
      "|    time_elapsed         | 39         |\n",
      "|    total_timesteps      | 18432      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00279586 |\n",
      "|    clip_fraction        | 0.046      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.46      |\n",
      "|    explained_variance   | 0.0233     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 4.23       |\n",
      "|    n_updates            | 230        |\n",
      "|    policy_gradient_loss | -0.000276  |\n",
      "|    value_loss           | 112        |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 463          |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 44           |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024576313 |\n",
      "|    clip_fraction        | 0.0292       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.474       |\n",
      "|    explained_variance   | -0.0022      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 96.9         |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.00124     |\n",
      "|    value_loss           | 111          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 465          |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 48           |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030895788 |\n",
      "|    clip_fraction        | 0.0298       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.466       |\n",
      "|    explained_variance   | -0.0685      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 77.4         |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -0.00183     |\n",
      "|    value_loss           | 108          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 469          |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 52           |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036500762 |\n",
      "|    clip_fraction        | 0.0383       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.468       |\n",
      "|    explained_variance   | 0.0154       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 36.5         |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | -0.00308     |\n",
      "|    value_loss           | 111          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 469         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 56          |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004459681 |\n",
      "|    clip_fraction        | 0.0292      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.458      |\n",
      "|    explained_variance   | 0.0198      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 47.9        |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.00254    |\n",
      "|    value_loss           | 96          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 467         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 61          |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007037447 |\n",
      "|    clip_fraction        | 0.0347      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.467      |\n",
      "|    explained_variance   | 0.0289      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 171         |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.00108    |\n",
      "|    value_loss           | 145         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 465          |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 66           |\n",
      "|    total_timesteps      | 30720        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040128725 |\n",
      "|    clip_fraction        | 0.101        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.448       |\n",
      "|    explained_variance   | 0.0557       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 68.6         |\n",
      "|    n_updates            | 290          |\n",
      "|    policy_gradient_loss | -0.00709     |\n",
      "|    value_loss           | 106          |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x219815591f0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=30000)  # train the agent (using the environment) for 30000 timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
